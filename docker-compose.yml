---
# ----------------------------------------------------------------------------------------
# -- Docs: https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker --
# ----------------------------------------------------------------------------------------
version: "3.6"
volumes:
    shared-workspace:
        name: "hadoop-distributed-file-system"
        driver: local

services:
#-----------
# -- HDFS --
#-----------
    namenode:
        image: bde2020/hadoop-namenode:1.1.0-hadoop2.8-java8
        container_name: namenode
        volumes:
            - ./data/namenode:/hadoop/dfs/name
        environment:
            - CLUSTER_NAME=test
        env_file:
            - ./hadoop.env
        ports:
            - 9870:50070
            - 9000:9000

    datanode:
        image: bde2020/hadoop-datanode:1.1.0-hadoop2.8-java8
        depends_on: 
            - namenode
        volumes:
            - ./data/datanode:/hadoop/dfs/data
        env_file:
            - ./hadoop.env
        ports:
            - 9875:50075

    hue:
        image: bde2020/hdfs-filebrowser:3.11
        ports:
            - 8089:8088
        environment:
            - NAMENODE_HOST=namenode
#------------
# -- Spark --
#------------
    spark-master:
        image: andreper/spark-master:3.0.0
        container_name: spark-master
        ports:
            - 8080:8080
            - 7077:7077
        volumes:
            - shared-workspace:/opt/workspace
        env_file:
            - ./hadoop.env

    spark-worker-1:
        image: andreper/spark-worker:3.0.0
        container_name: spark-worker-1
        environment:
            - SPARK_WORKER_CORES=1
            - SPARK_WORKER_MEMORY=1024m
        ports:
            - 8081:8081
        volumes:
            - shared-workspace:/opt/workspace
        depends_on:
            - spark-master
        env_file:
            - ./hadoop.env

    spark-worker-2:
        image: andreper/spark-worker:3.0.0
        container_name: spark-worker-2
        environment:
            - SPARK_WORKER_CORES=1
            - SPARK_WORKER_MEMORY=1024m
        ports:
            - 8082:8081
        volumes:
            - shared-workspace:/opt/workspace
        depends_on:
            - spark-master
        env_file:
            - ./hadoop.env

    jupyterlab:
        image: andreper/jupyterlab:3.0.0-spark-3.0.0
        container_name: jupyterlab
        ports:
          - 8890:8888
          - 4040:4040
        volumes:
          - shared-workspace:/opt/workspace
#------------
# -- Kafka --
#------------
    zookeeper:
      image: confluentinc/cp-zookeeper:latest
      environment:
        ZOOKEEPER_CLIENT_PORT: 2181
        ZOOKEEPER_TICK_TIME: 2000
  
    broker:
      image: confluentinc/cp-kafka:latest
      depends_on:
        - zookeeper
      environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    
    middle-rate:
        build: ./middle_rate
        depends_on:
            - broker
        environment:
            KAFKA_BROKER_URL: broker:9092
            TRANSACTIONS_TOPIC: queueing.crude
            TRANSACTIONS_PER_SECOND: 1000
        volumes:
            - ./scripts:/usr/scripts
            - ./debug:/usr/debug
        
#--------------
# -- Airflow --
#--------------
    airflow:
      build: ./airflow
      ports:
        - 8088:8080
      environment:   
        - FERNET_KEY="bCH80KIHfj-_UAt1b4gaFPJvQCMF73FQRFJGtOhFA-M="
      volumes:
        - ./dags/:/usr/local/airflow/dags
        - ./scripts/:/usr/local/scripts
        
networks:
    default:
        external:
            name: kafka-network